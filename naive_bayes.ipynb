{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/cihansariyildiz/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/cihansariyildiz/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/cihansariyildiz/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  label                                               text\n",
      "0   ham  enron methanol meter 988291 follow note gave m...\n",
      "1   ham  hpl nom januari 9 2001 see attach file hplnol ...\n",
      "2   ham  neon retreat ho ho ho around wonder time year ...\n",
      "3  spam  photoshop window offic cheap main trend aba da...\n",
      "4   ham  indian spring deal book teco pvr revenu unders...\n",
      "0.9845360824742269\n",
      "[[1107   14]\n",
      " [  10  421]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         ham       0.99      0.99      0.99      1121\n",
      "        spam       0.97      0.98      0.97       431\n",
      "\n",
      "    accuracy                           0.98      1552\n",
      "   macro avg       0.98      0.98      0.98      1552\n",
      "weighted avg       0.98      0.98      0.98      1552\n",
      "\n",
      "spam\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tokenize as tk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfTransformer\n",
    "## bayes\n",
    "from sklearn.naive_bayes import  MultinomialNB\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Text Normalization\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer(\"english\")\n",
    "\n",
    "data = pd.read_csv('spam_ham_dataset.csv')\n",
    "data = data[['label', 'text']]\n",
    "data = data.dropna() # remove rows with missing values (if any)\n",
    "\n",
    "\n",
    "# Text Normalization\n",
    "def normalize_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^a-z0-9]', ' ', text)\n",
    "    return text\n",
    "# Tokenization\n",
    "def tokenize_text(text):\n",
    "    return word_tokenize(text)\n",
    "\n",
    "# Stopword Removal\n",
    "def remove_stopwords(text):\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    return [word for word in text if word not in stop_words]\n",
    "\n",
    "\n",
    "\n",
    "# Preprocessing\n",
    "\n",
    "data['text'] = data['text'].apply(normalize_text)\n",
    "data['text'] = data['text'].apply(tokenize_text)\n",
    "data['text'] = data['text'].apply(remove_stopwords)\n",
    "\n",
    " ## stem and lemmetize\n",
    "data['text'] = data['text'].apply(lambda x: [stemmer.stem(y) for y in x])\n",
    "data['text'] = data['text'].apply(lambda x: [WordNetLemmatizer().lemmatize(y) for y in x])\n",
    "\n",
    "# remove the subject\n",
    "data['text'] = data['text'].apply(lambda x: x[1:])\n",
    "\n",
    "data.head()\n",
    "\n",
    "# Join words back into a string\n",
    "data['text'] = data['text'].apply(lambda x: ' '.join(x))\n",
    "print(data.head())\n",
    "\n",
    "X = data['text']\n",
    "y = data['label']\n",
    "\n",
    "# # split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3,random_state=42)\n",
    "\n",
    "# count vectorizer\n",
    "count_vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
    "X_train_counts = count_vectorizer.fit_transform(X_train)\n",
    "X_test_counts = count_vectorizer.transform(X_test)\n",
    "\n",
    "# tf-idf transformer\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
    "X_test_tfidf = tfidf_transformer.transform(X_test_counts)\n",
    "\n",
    "# bayes\n",
    "clf = MultinomialNB(alpha=0.01)\n",
    "clf.fit(X_train_tfidf, y_train)\n",
    "\n",
    "# predict\n",
    "y_pred = clf.predict(X_test_tfidf)\n",
    "\n",
    "# accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_test, y_pred))\n",
    "\n",
    "# # confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# # classification report\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "\n",
    "\n",
    "def predict_spam(text):\n",
    "    text = normalize_text(text)\n",
    "    text = tokenize_text(text)\n",
    "    text = remove_stopwords(text)\n",
    "    text = [stemmer.stem(y) for y in text]\n",
    "    text = [WordNetLemmatizer().lemmatize(y) for y in text]\n",
    "    text = ' '.join(text)\n",
    "    text = count_vectorizer.transform([text])\n",
    "    text = tfidf_transformer.transform(text)\n",
    "    return clf.predict(text)[0]\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fresh",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
